{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2I5CvB/LhIfaWyLQ5+oRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orcunmadran/hu-bby261-2024/blob/main/nlpBasicsNLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.org/project/nltk/\n",
        "# https://www.nltk.org/"
      ],
      "metadata": {
        "id": "1zEkPVjVBZ9F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "rnST2TdAAqcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc37f12a-d2e2-4e66-9586-83872f14c139"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "source": [
        "# Paragraf metniniz\n",
        "paragraf_metni = \"Araştırmanın ikinci aşamasında yeni anahtar kelimelerin oluşturulmasını sağlayan makine öğrenmesi algoritması, Word2Vec olarak adlandırılan ve kelimelerin vektör uzayında birbirlerine olan uzaklıklarını belirleyebildiğimiz bir algoritmadır. Word2Vec, bilgisayar biliminde doğal dil işleme olarak ifade edilen çalışma alanında kullanılan bir teknik olarak da ifade edilebilir. Geniş bir derlem içindeki kelimeler arası anlamsal benzerlikleri vektörler aracılığıyla tanımlama imkânı sağlayan bir yapıya sahiptir. Word2Vec algoritması 2013 yılında Google çalışanı Tomas Mikolov önderliğindeki bir grup araştırmacı tarafından yaratılmıştır (Mikolov ve diğerleri, 2013).\"\n",
        "\n",
        "# 10 farklı konu başlığı\n",
        "konu_basliklari = [\n",
        "    \"Makine öğrenmesi\",\n",
        "    \"Yapay zeka\",\n",
        "    \"Derin öğrenme\",\n",
        "    \"Doğal dil işleme\",\n",
        "    \"Bilgisayar görüşü\",\n",
        "    \"Veri bilimi\",\n",
        "    \"İstatistik\",\n",
        "    \"Programlama\",\n",
        "    \"Robotik\",\n",
        "    \"Açık Eğitim Kaynakları\",\n",
        "]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QsQMakTHAwXF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "source": [
        "stop_words = set(stopwords.words('turkish'))  # Türkçe için stop words'leri kullanın\n",
        "\n",
        "def metni_on_isle(metin):\n",
        "    tokens = word_tokenize(metin.lower())  # Metni küçük harfe çevirin ve token'lara ayırın\n",
        "    tokens = [w for w in tokens if not w in stop_words]  # Stop words'leri kaldırın\n",
        "    return \" \".join(tokens)  # Token'ları tekrar bir metin haline getirin\n",
        "\n",
        "paragraf_metni_islenmis = metni_on_isle(paragraf_metni)\n",
        "konu_basliklari_islenmis = [metni_on_isle(baslik) for baslik in konu_basliklari]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QkjtzvH0AyuR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tum_metinler = [paragraf_metni_islenmis] + konu_basliklari_islenmis\n",
        "tfidf_matrisi = vectorizer.fit_transform(tum_metinler)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xb8ZO-yPA1v-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "source": [
        "paragraf_vektoru = tfidf_matrisi[0]\n",
        "konu_basliklari_vektorleri = tfidf_matrisi[1:]\n",
        "\n",
        "benzerlikler = cosine_similarity(paragraf_vektoru, konu_basliklari_vektorleri)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "frGO1kEGA4U2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "source": [
        "en_alakali_indeks = benzerlikler.argmax()\n",
        "en_alakali_konu_basligi = konu_basliklari[en_alakali_indeks]\n",
        "\n",
        "print(f\"Paragraf metni en çok '{en_alakali_konu_basligi}' konu başlığı ile alakalı.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls6Uo35kA8bs",
        "outputId": "e20a9b5f-26eb-436f-9d20-3d7eabf16dac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraf metni en çok 'Doğal dil işleme' konu başlığı ile alakalı.\n"
          ]
        }
      ]
    }
  ]
}